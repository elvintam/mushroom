---
title: 'Data Science: Capstone CYO Project - Mushroom'
author: "Elvin Tam"
date: "10 June 2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r package_data, include=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(DataExplorer)) install.packages("DataExplorer", repos = "http://cran.us.r-project.org")
if(!require(rpart.plot)) install.packages("rpart.plot", repos = "http://cran.us.r-project.org")


library(tidyverse)
library(caret)
library(data.table)
library(DataExplorer)
library(knitr)
library(rpart.plot)

ifelse(file.exists(".\\data\\agaricus-lepiota.data"),
  { 
    dl <- ".\\data\\agaricus-lepiota.data"
  },
  {
    dl <- tempfile()
    download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data", dl)
  })

mushroom <- fread(text = readLines(dl), header = FALSE, stringsAsFactors = TRUE,
                 col.names = c(
                   "class","cap_shape",
                   "cap_surface","cap_color",
                   "bruises","odor","gill_attachment",
                   "gill_spacing","gill_size","gill_color",
                   "stalk_shape","stalk_root","stalk_surface_above_ring",
                   "stalk_surface_below_ring","stalk_color_above_ring",
                   "stalk_color_below_ring","veil_type","veil_color",
                   "ring_number","ring_type","spore_print_color",
                   "population","habitat"))

mushroom <- as.data.frame(mushroom)
rm(dl)
```

```{r mushroompic, echo = FALSE}
# All defaults
include_graphics("mushroom.jpg")
```
CREDIT: GETTY IMAGES

# Introduction

In this report, our goal is to predict the edibility (class: edible / poisonous) of mushroom basing on attribution information. Data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family (pp. 500-525). The reason of selecting this dataset is that this problem is related to classification which is a large part of application in data science. And, it is also a complement to project – MovieLens that we can cover each part of what we have learnt from the course.

The mushroom dataset has already been well formatted. Process of data cleaning is only removing 2 attributes prior to splitting the data to training set and test set. 10 algorithms are applied and an ensemble model combining the prior 10 different algorithms to see if it can provide improvement to our predictions.  

## 1. Data Cleaning

Mushroom data set contains 23 columns of 1 class and 22 attributes related to cap, bruises, odor, gill, stalk, veil, ring, spore color, population and habitat of 8,124 observations. 

```{r str, echo=FALSE}
str(mushroom)
summary(mushroom)
```

According to description from the source, there is data missing in the attribute of stalk_root. The missing data point is marked “?” from the source already. On the other hand, veil_type is a constant. Both stalk_root and veil_type are removed before we start data exploration & modeling.

```{r remove, echo=TRUE}
mushroom <- mushroom %>% select(-veil_type, -stalk_root)
```

## 2. Data Exploration

We will use Data Explorer package in the process of data exploration. By using this package, it provides a standardized method to get the insights from the dataset. 

From below 2 charts, we can see that the mushroom data is discrete with no data missing.

```{r dataExplorer1, echo=FALSE}
plot_intro(mushroom)
plot_missing(mushroom)
```

From below frequency and percentage charts, we can see how many observations belong to each category in each attribute and among those how many are edible or poisonous. In class which we are going to predict, we can say the feature is roughly equal distributed. However, observations are mainly clustered in one category in gill_attachment, gill_spacing, veil_color and ring_number.

```{r dataExplorer2, echo=FALSE}
plot_bar(mushroom, nrow = 3L, ncol = 4L)
plot_bar(mushroom, by = "class", nrow = 3L, ncol = 4L)
```

From correlation matrix (filtered category less than 5), there is high correlation between veil_color and gill_attachment, stalk_surface_above_ring and bruises.

```{r dataExplorer3, echo=FALSE}
plot_correlation(mushroom, maxcat = 5L)
```

## 3. Modeling Approach

```{r datasplit, echo=FALSE, warning=FALSE}

  set.seed(12345, sample.kind="Rounding")
  test_index <- createDataPartition(y = mushroom$class, times = 1,
                                    p = 0.2, list = FALSE)
  test_set <- mushroom[test_index,]
  train_set <- mushroom[-test_index,]
  
  rm(test_index)
```

We will use 10 algorithms and 1 ensemble model to evaluate if this can provide improvement to our predictions. Algorithms are listed below.

3-1.	glm <br>
3-2.	lda <br>
3-3.	Naïve Bayes <br>
3-4.	svmLinear <br>
3-5.	Classification Model <br>
3-6.	knn <br>
3-7.	gamLoess <br>
3-8.	multinom <br>
3-9.	rf <br>
3-10.	adaboost <br>
3-11.	ensemble <br>

### 3-1. glm

Generalized Linear Model (glm) is the most common and simple model. This is the starting point of our modeling. Using Caret package, we can simply apply 10 different algorithms in a standardized way. 

In glm model, we get warning message of "Warning: glm.fit: algorithm did not converge", "Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred" and "prediction from a rank-deficient fit may be misleading". 

```{r glm1, echo=TRUE, warning=FALSE}

fit_glm <- train(class ~ ., method = "glm", data = train_set)

s <- summary(fit_glm)

s
```

From the above Summary of Coefficients, we find that 8 coefficients are not defined (NA) because of singularities. They are listed below. 

```{r glm2, echo=TRUE}
s$aliased[which(s$aliased == TRUE)]
```

These 8 coefficients belong to 6 attributes. We will remove them and run the glm algorithm again.

```{r glm3, echo=TRUE, warning=FALSE}
train_set <- train_set %>% select(-stalk_color_above_ring, -stalk_color_below_ring, 
                                  -veil_color, -ring_number, -ring_type, -spore_print_color)
test_set <- test_set %>% select(-stalk_color_above_ring, -stalk_color_below_ring, 
                                -veil_color, -ring_number, -ring_type, -spore_print_color)

fit_glm <- train(class ~ ., method = "glm", data = train_set)

s <- summary(fit_glm)

s

s$aliased[which(s$aliased == TRUE)]

```

glm results are tabulated below.

```{r glm4, echo=FALSE, warning=FALSE}
y_hat_glm <- predict(fit_glm, test_set)

cm <- confusionMatrix(y_hat_glm, test_set$class)

model_results <- tibble(Method = "glm",
                        Accuracy = cm$overall["Accuracy"],
                        Kappa = cm$overall["Kappa"],
                        Sensitivity = cm$byClass["Sensitivity"],
                        Specificity = cm$byClass["Specificity"])

model_results %>% knitr::kable()


### data recovery

set.seed(12345, sample.kind="Rounding")
test_index <- createDataPartition(y = mushroom$class, times = 1,
                                    p = 0.2, list = FALSE)
test_set <- mushroom[test_index,]
train_set <- mushroom[-test_index,]
  
rm(test_index)

```

### 3-2. lda

```{r lda, echo=TRUE, warning=FALSE}

fit_lda <- train(class ~ ., method = "lda", data = train_set)

s <- summary(fit_lda)

s
```

```{r lda2, echo=FALSE}
y_hat_lda <- predict(fit_lda, test_set)

cm <- confusionMatrix(y_hat_lda, test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "lda",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"]))

model_results %>% knitr::kable()
```

### 3-3.	Naïve Bayes

```{r nb1, echo=TRUE, warning=FALSE}

fit_nb <- train(class ~ ., method = "naive_bayes", data = train_set)

summary(fit_nb)

```

```{r nb2, echo=FALSE}
y_hat_nb <- predict(fit_nb, test_set)

cm <- confusionMatrix(y_hat_nb, test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "navie bayes",
                        Accuracy = cm$overall["Accuracy"],
                        Kappa = cm$overall["Kappa"],
                        Sensitivity = cm$byClass["Sensitivity"],
                        Specificity = cm$byClass["Specificity"]))

model_results %>% knitr::kable()
```

### 3-4.	svmLinear

Support Vector Machine

```{r svmLinear1, echo=TRUE, warning=FALSE}

fit_svmLinear <- train(class ~ ., method = "svmLinear", data = train_set)
fit_svmLinear["finalModel"]

```

```{r svmLinear2, echo=FALSE}
y_hat_svmLinear <- predict(fit_svmLinear, test_set)

cm <- confusionMatrix(y_hat_svmLinear, test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "svmLinear",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"]))

model_results %>% knitr::kable()
```

### 3-5.	Classification Model

```{r CM1, echo=TRUE, warning=FALSE}

fit_rpart <- train(class ~ ., method = "rpart", data = train_set)
fit_rpart["finalModel"]

rpart.plot(fit_rpart$finalModel)

```

```{r CM2, echo=FALSE}

y_hat_rpart <- predict(fit_rpart, test_set)

cm <- confusionMatrix(y_hat_rpart, test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "rpart",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"]))

model_results %>% knitr::kable()
```

### 3-6.	knn

```{r knn1, echo=TRUE, warning=FALSE}

fit_knn <- train(class ~ ., method = "knn", data = train_set)
fit_knn["finalModel"]

```

```{r knn2, echo=FALSE}
y_hat_knn <- predict(fit_knn, test_set)

cm <- confusionMatrix(y_hat_knn, test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "knn",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"]))

model_results %>% knitr::kable()
```

### 3-7.	gamLoess

```{r gamLoess1, echo=TRUE, message=FALSE, warning=FALSE}

fit_gamLoess <- train(class ~ ., method = "gamLoess", data = train_set)

s <- summary(fit_gamLoess)

s$parametric.anova
```

```{r gamLoess2, echo=FALSE}
y_hat_gamLoess <- predict(fit_gamLoess, test_set)

cm <- confusionMatrix(y_hat_gamLoess, test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "gamLoess",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"]))

model_results %>% knitr::kable()
```

### 3-8.	multinom

```{r multinom1, message=FALSE, warning=FALSE, include=FALSE}

fit_multinom <- train(class ~ ., method = "multinom", data = train_set)
```

```{r multinom2, echo=TRUE, warning=FALSE}
fit_multinom["finalModel"]
```

```{r multinom3, echo=FALSE}

y_hat_multinom <- predict(fit_multinom, test_set)

cm <- confusionMatrix(y_hat_multinom, test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "multinom",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"]))

model_results %>% knitr::kable()
```

### 3-9.	rf

Random Forest
```{r rf1, echo=TRUE, warning=FALSE}
fit_rf <- train(class ~ ., method = "Rborist", data = train_set)
fit_rf["finalModel"]
```

```{r rf2, echo=FALSE}

y_hat_rf <- predict(fit_rf, test_set)

cm <- confusionMatrix(y_hat_rf, test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "rf",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"]))

model_results %>% knitr::kable()
```

### 3-10.	adaboost

```{r adaboost, echo=TRUE, warning=FALSE}

fit_adaboost <- train(class ~ ., method = "adaboost", data = train_set)
fit_adaboost["finalModel"]

```

```{r adabost2, echo=FALSE}
y_hat_adaboost <- predict(fit_adaboost, test_set)

cm <- confusionMatrix(y_hat_adaboost, test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "adaboost",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"]))

model_results %>% knitr::kable()
```

### 3-11.	ensemble

```{r ensemble1, echo=TRUE, warning=FALSE}

y_hat_results <- bind_cols(y_hat_glm, y_hat_lda, y_hat_nb, 
                           y_hat_svmLinear, y_hat_rpart,
                           y_hat_knn, y_hat_gamLoess, y_hat_multinom, 
                           y_hat_rf, y_hat_adaboost)

y_hat_ensemble <- ifelse(rowMeans(y_hat_results == "e") >0.5, "e", "p")

```

```{r ensemble2, echo=FALSE}
cm <- confusionMatrix(as.factor(y_hat_ensemble), test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "ensemble",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"]))

model_results %>% knitr::kable()

```


So among all classification model Random Forest Classification has highest accuracy score = 99.75%.

# Result
a results section that presents the modeling results and discusses the model performance

# Conclusion
a conclusion section that gives a brief summary of the report, its limitations and future work
