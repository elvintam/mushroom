---
title: 'Data Science: Capstone CYO Project - Mushroom'
author: "Elvin Tam"
date: "10 June 2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r package_data, include=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(DataExplorer)) install.packages("DataExplorer", repos = "http://cran.us.r-project.org")
if(!require(rpart.plot)) install.packages("rpart.plot", repos = "http://cran.us.r-project.org")


library(tidyverse)
library(caret)
library(data.table)
library(DataExplorer)
library(knitr)
library(rpart.plot)

ifelse(file.exists(".\\data\\agaricus-lepiota.data"),
  { 
    dl <- ".\\data\\agaricus-lepiota.data"
  },
  {
    dl <- tempfile()
    download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data", dl)
  })

mushroom <- fread(text = readLines(dl), header = FALSE, stringsAsFactors = TRUE,
                 col.names = c(
                   "class","cap_shape",
                   "cap_surface","cap_color",
                   "bruises","odor","gill_attachment",
                   "gill_spacing","gill_size","gill_color",
                   "stalk_shape","stalk_root","stalk_surface_above_ring",
                   "stalk_surface_below_ring","stalk_color_above_ring",
                   "stalk_color_below_ring","veil_type","veil_color",
                   "ring_number","ring_type","spore_print_color",
                   "population","habitat"))

mushroom <- as.data.frame(mushroom)
rm(dl)
```

```{r mushroompic, echo = FALSE}
# All defaults
include_graphics("mushroom.jpg")
```
CREDIT: GETTY IMAGES

# Introduction

In this report, our goal is to predict the edibility (class: edible / poisonous) of mushroom basing on attribution information. Data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family (pp. 500-525). The reason of selecting this dataset is that this problem is related to classification which is a large part of application in data science. And, it is also a complement to project – MovieLens that we can cover each part of what we have learnt from the course.

The mushroom dataset has already been well formatted. Process of data cleaning is only removing 2 attributes prior to splitting the data to training set and test set. 10 algorithms are applied and an ensemble model combining the prior 10 different algorithms to see if it can provide improvement to our predictions.  

## 1. Data Cleaning

Mushroom data set contains 23 columns of 1 class and 22 attributes related to cap, bruises, odor, gill, stalk, veil, ring, spore color, population and habitat of 8,124 observations. 

```{r str, echo=FALSE}
str(mushroom)
summary(mushroom)
```

According to description from the source, there is data missing in the attribute of stalk_root. The missing data point is marked “?” from the source already. On the other hand, veil_type is a constant. Both stalk_root and veil_type are removed before we start data exploration & modeling.

```{r remove, echo=TRUE}
mushroom <- mushroom %>% select(-veil_type, -stalk_root)
```

## 2. Data Exploration

We will use Data Explorer package in the process of data exploration. By using this package, it provides a standardized method to get the insights from the dataset. 

From below 2 charts, we can see that the mushroom data is discrete with no data missing.

```{r dataExplorer1, echo=FALSE}
plot_intro(mushroom)
plot_missing(mushroom)
```

From below frequency and percentage charts, we can see how many observations belong to each category in each attribute and among those how many are edible or poisonous. In class which we are going to predict, we can say the feature is roughly equal distributed. However, observations are mainly clustered in one category in gill_attachment, gill_spacing, veil_color and ring_number.

```{r dataExplorer2, echo=FALSE}
plot_bar(mushroom, nrow = 3L, ncol = 4L)
plot_bar(mushroom, by = "class", nrow = 3L, ncol = 4L)
```

From correlation matrix (filtered category less than 5), there is high correlation between veil_color and gill_attachment, stalk_surface_above_ring and bruises.

```{r dataExplorer3, echo=FALSE}
plot_correlation(mushroom, maxcat = 5L)
```

## 3. Modeling Approach

```{r datasplit, echo=FALSE, warning=FALSE}

  set.seed(12345, sample.kind="Rounding")
  test_index <- createDataPartition(y = mushroom$class, times = 1,
                                    p = 0.2, list = FALSE)
  test_set <- mushroom[test_index,]
  train_set <- mushroom[-test_index,]
  
  rm(test_index)
```

We will use 10 algorithms and 1 ensemble model to evaluate if this can provide improvement to our predictions. Algorithms are listed below.

3-1.	GLM <br>
3-2.	LDA <br>
3-3.	Naïve Bayes <br>
3-4.	svmLinear <br>
3-5.	KNN <br>
3-6.	gamLoess <br>
3-7.	Multinom <br>
3-8.	Classification Model <br>
3-9.	Random Forest <br>
3-10.	Adaboost <br>
3-11.	Ensemble <br>

### 3-1. GLM

Generalized Linear Model (GLM) is the most common and general model. This is the starting point of our modeling. Using Caret package, we can simply apply 10 different algorithms in a standardized way. 

When we run the GLM algorithms, we get warning message of "Warning: glm.fit: algorithm did not converge", "Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred" and "prediction from a rank-deficient fit may be misleading". 

```{r glm1, echo=TRUE, warning=FALSE}
start_time <- Sys.time()
fit_glm <- train(class ~ ., method = "glm", data = train_set)
time_diff <- Sys.time() - start_time

s <- summary(fit_glm)

s
```

From the above Summary of Coefficients, we find that 8 coefficients are not defined (NA) because of singularities. They are listed below. 

```{r glm2, echo=TRUE}
s$aliased[which(s$aliased == TRUE)]
```

These 8 coefficients belong to 6 attributes. We will remove them and run the glm algorithm again.

```{r glm3, echo=TRUE, warning=FALSE}
train_set <- train_set %>% select(-stalk_color_above_ring, -stalk_color_below_ring, 
                                  -veil_color, -ring_number, -ring_type, -spore_print_color)
test_set <- test_set %>% select(-stalk_color_above_ring, -stalk_color_below_ring, 
                                -veil_color, -ring_number, -ring_type, -spore_print_color)

start_time <- Sys.time()
fit_glm <- train(class ~ ., method = "glm", data = train_set)
time_diff <- Sys.time() - start_time

s <- summary(fit_glm)

s

s$aliased[which(s$aliased == TRUE)]

```

GLM results are tabulated below.

```{r glm4, echo=FALSE, warning=FALSE}
y_hat_glm <- predict(fit_glm, test_set)

cm <- confusionMatrix(y_hat_glm, test_set$class)

model_results <- tibble(Method = "glm",
                        Accuracy = cm$overall["Accuracy"],
                        Kappa = cm$overall["Kappa"],
                        Sensitivity = cm$byClass["Sensitivity"],
                        Specificity = cm$byClass["Specificity"],
                        Train_Time = time_diff)

model_results %>% knitr::kable()


### data recovery

set.seed(12345, sample.kind="Rounding")
test_index <- createDataPartition(y = mushroom$class, times = 1,
                                    p = 0.2, list = FALSE)
test_set <- mushroom[test_index,]
train_set <- mushroom[-test_index,]
  
rm(test_index)

```

### 3-2. LDA

Linear Discriminant Analysis (LDA) is an algorithm for predictive classification modeling problems. LDA makes predictions by estimating the probability that a new set of inputs belongs to each class. The class that gets the highest probability is the output class and a prediction is made.

```{r lda, echo=TRUE, warning=FALSE}
start_time <- Sys.time()
fit_lda <- train(class ~ ., method = "lda", data = train_set)
time_diff <- Sys.time() - start_time

s <- summary(fit_lda)

s
```

```{r lda2, echo=FALSE}
y_hat_lda <- predict(fit_lda, test_set)

cm <- confusionMatrix(y_hat_lda, test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "lda",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"],
                              Train_Time = time_diff))

model_results %>% knitr::kable()
```

### 3-3.	Naïve Bayes

Naïve Bayes algorithm is based on Bayes theorem and used for solving classification problems. It is a probabilistic classifier basing on the probability of an object to make prediction.

```{r nb1, echo=TRUE, warning=FALSE}
start_time <- Sys.time()
fit_nb <- train(class ~ ., method = "naïve_bayes", data = train_set)
time_diff <- Sys.time() - start_time

summary(fit_nb)

```

```{r nb2, echo=FALSE}
y_hat_nb <- predict(fit_nb, test_set)

cm <- confusionMatrix(y_hat_nb, test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "navie bayes",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"],
                              Train_Time = time_diff))

model_results %>% knitr::kable()
```

### 3-4.	svmLinear

svmLinear stands for Support Vector Machine (SVM) Linear Model. It fits a linear SVM model by identifying the optimal decision boundary that separates data points from different classes, and then predicts the class of new observations based on this separation boundary.

```{r svmLinear1, echo=TRUE, warning=FALSE}
start_time <- Sys.time()
fit_svmLinear <- train(class ~ ., method = "svmLinear", data = train_set)
time_diff <- Sys.time() - start_time

fit_svmLinear["finalModel"]

```

```{r svmLinear2, echo=FALSE}
y_hat_svmLinear <- predict(fit_svmLinear, test_set)

cm <- confusionMatrix(y_hat_svmLinear, test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "svmLinear",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"],
                              Train_Time = time_diff))

model_results %>% knitr::kable()
```

### 3-5.	KNN

The k-nearest neighbors (KNN) algorithm predicts the outcome of a new observation by comparing it to k similar cases in the training data set. The best tune here is k = 5.

```{r knn1, echo=TRUE, warning=FALSE}
start_time <- Sys.time()
fit_knn <- train(class ~ ., method = "knn", data = train_set)
time_diff <- Sys.time() - start_time

fit_knn["finalModel"]
fit_knn$bestTune

```

```{r knn2, echo=FALSE}
y_hat_knn <- predict(fit_knn, test_set)

cm <- confusionMatrix(y_hat_knn, test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "knn",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"],
                              Train_Time = time_diff))

model_results %>% knitr::kable()
```

### 3-6.	gamLoess

gamLoess stands for Generalized Additive Model using LOESS (Local weighted regression). Comparing to the bin smoother approach with constant assumptions in KNN, Loess considers larger window size (span = 0.5 here) with fitting a line within that window than with a constant. 

```{r gamLoess1, echo=TRUE, message=FALSE, warning=FALSE}
start_time <- Sys.time()
fit_gamLoess <- train(class ~ ., method = "gamLoess", data = train_set)
time_diff <- Sys.time() - start_time

fit_gamLoess$bestTune
```

```{r gamLoess2, echo=FALSE}
y_hat_gamLoess <- predict(fit_gamLoess, test_set)

cm <- confusionMatrix(y_hat_gamLoess, test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "gamLoess",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"],
                              Train_Time = time_diff))

model_results %>% knitr::kable()
```

### 3-7.	multinom

Multinomial Regression is an extension of the logistic regression. It is specially designed for the nominal data. The target dependent variable can have more than two classes. Although we only have two classes, we still apply the algorithm to check the result.

```{r multinom1, message=FALSE, warning=FALSE, include=FALSE}
start_time <- Sys.time()
fit_multinom <- train(class ~ ., method = "multinom", data = train_set)
time_diff <- Sys.time() - start_time
```

```{r multinom2, echo=TRUE, warning=FALSE}
fit_multinom["finalModel"]
```

```{r multinom3, echo=FALSE}

y_hat_multinom <- predict(fit_multinom, test_set)

cm <- confusionMatrix(y_hat_multinom, test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "multinom",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"],
                              Train_Time = time_diff))

model_results %>% knitr::kable()
```

### 3-8.	Classification Model

Classification Model is basing decision tree. It works by repeatedly partitioning data into multiple sub-spaces. The outcome in each final sub-space is as homogeneous as possible. Both text and visualized binary tree are presented here. 

```{r CM1, echo=TRUE, warning=FALSE}
start_time <- Sys.time()
fit_rpart <- train(class ~ ., method = "rpart", data = train_set)
time_diff <- Sys.time() - start_time

fit_rpart["finalModel"]

rpart.plot(fit_rpart$finalModel)

```

```{r CM2, echo=FALSE}

y_hat_rpart <- predict(fit_rpart, test_set)

cm <- confusionMatrix(y_hat_rpart, test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "classif. model",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"],
                              Train_Time = time_diff))

model_results %>% knitr::kable()
```

### 3-9.	Random Forest

Random Forest algorithm addresses the shortcomings of decision tree with a special type of bagging to the data and bootstrap sampling at each split (bootstrap aggregating). This means that at each splitting step of the tree algorithm, a random sample of n predictors is chosen as split candidates from the full set of the predictors. As suggested in the course, we used "Rborist" method rather than "rf" method because of time efficiency. Variable Importance shows that Odorn = 1 is the most important. This is in-line with the decision tree in Classification Model.

```{r rf1, echo=TRUE, warning=FALSE}
start_time <- Sys.time()
fit_rf <- train(class ~ ., method = "Rborist", data = train_set)
time_diff <- Sys.time() - start_time

fit_rf$bestTune

varImp(fit_rf)
```

```{r rf2, echo=FALSE}

y_hat_rf <- predict(fit_rf, test_set)

cm <- confusionMatrix(y_hat_rf, test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "random forest",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"],
                              Train_Time = time_diff))

model_results %>% knitr::kable()
```

### 3-10.	Adaboost

Instead of growing tree randomly, Adaptive Boosting algorithm grows trees using information from previously grown trees, with the aim to minimize the error of the previous models.

```{r adaboost, echo=TRUE, warning=FALSE}
start_time <- Sys.time()
fit_adaboost <- train(class ~ ., method = "adaboost", data = train_set)
time_diff <- Sys.time() - start_time

fit_adaboost["finalModel"]

varImp(fit_adaboost)

```

```{r adabost2, echo=FALSE}
y_hat_adaboost <- predict(fit_adaboost, test_set)

cm <- confusionMatrix(y_hat_adaboost, test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "adaboost",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"],
                              Train_Time = time_diff))

model_results %>% knitr::kable()
```

### 3-11.	ensemble

Ensemble model combines the results from previous 10 algorithms. If more than 50% algorithms predict edible, it will predict edible. 

```{r ensemble1, echo=TRUE, message=FALSE, warning=FALSE}
start_time <- Sys.time()
y_hat_results <- bind_cols(y_hat_glm, y_hat_lda, y_hat_nb, 
                           y_hat_svmLinear, y_hat_rpart,
                           y_hat_knn, y_hat_gamLoess, y_hat_multinom, 
                           y_hat_rf, y_hat_adaboost)
time_diff <- Sys.time() - start_time

y_hat_ensemble <- ifelse(rowMeans(y_hat_results == "e") >0.5, "e", "p")

```

```{r ensemble2, echo=FALSE}
cm <- confusionMatrix(as.factor(y_hat_ensemble), test_set$class)

model_results <- rbind(model_results,
                       tibble(Method = "ensemble",
                              Accuracy = cm$overall["Accuracy"],
                              Kappa = cm$overall["Kappa"],
                              Sensitivity = cm$byClass["Sensitivity"],
                              Specificity = cm$byClass["Specificity"],
                              Train_Time = time_diff))

model_results %>% knitr::kable()

```

# Result

From the above table, the lowest accuracy is coming from Naïve Bayes followed by Classification Model and LDA. Other algorithms are all 100% accurate. The Kappa of these 3 algorithms are lower than 1 (from 0.88 to 0.99), which means that it is slightly affected by randomness. For Sensitivity, only Naïve Bayes is lower than 1. For Specificity, these 3 algorithms are lower than 100% but they are in high tier of 9x%.

In terms of performance (basing on my PC, 6 cores, 3.4GHz, 32G Ram), it takes more than 200 secs to finish the training process by Random Forest and more than 100 secs by Adaboost. The more trees created in the forest the lowest performance it is. KNN takes about 90 secs to finish because it best-tunes the results in CARET package. Ensemble model consumes less than 0.01 sec by only used the in RAM results from other algorithms with only one IF code. 

# Conclusion
a conclusion section that gives a brief summary of the report, its limitations and future work
